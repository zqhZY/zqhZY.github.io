<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="Zico">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="LSTM 句子相似度分析"/>
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="A Note"/>
  <!--Type page-->
  
      <meta property="og:type" content="article" />
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Title -->
  
  <title>LSTM 句子相似度分析 - A Note</title>


  <link rel="shortcut icon" href="https://hexo.io/icon/favicon-96x96.png">

  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css">

  <!----------------------------
  https://github.com/GallenHu/hexo-theme-Daily

 _____            _   _
|  __ \          (_) | |
| |  | |   __ _   _  | |  _   _
| |  | |  / _` | | | | | | | | |
| |__| | | (_| | | | | | | |_| |
|_____/   \__,_| |_| |_|  \__, |
                          __/ |
                         |___/

    --------------------------->

</head>


<body>

  <!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="https://hexo.io/logo.svg" alt="A Note" height="60">
        
      </a>
    </div>
    <!-- Navigation -->
    <nav class="navbar">
      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  Home
                
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  Archive
                
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            LSTM 句子相似度分析
            
          </h1>
          <p class="posted-on">
          2017-06-01
          </p>
          <div class="tags-links">
            
              
                <a href="/tags/DNN/" rel="tag">
                  DNN
                </a>
              
                <a href="/tags/NLP/" rel="tag">
                  NLP
                </a>
              
                <a href="/tags/句子相似度/" rel="tag">
                  句子相似度
                </a>
              
            
          </div>
        </div>
        <!-- Post Main Content -->
        <div class="entry-content has_line_number">
          <p><img src="http://upload-images.jianshu.io/upload_images/2159902-436935dc79f93b47.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="wordvector"></p>
<p> 使用句子中出现单词的Vector加权平均进行文本相似度分析虽然简单，但也有比较明显的缺点：没有考虑词序且词向量区别不明确。如下面两个句子：</p>
<ul>
<li>“北京的首都是中国”与“中国的首都是北京”的相似度为1。</li>
<li>“学习容易”和“学习困难”的相似度很容易也非常高。<a id="more"></a>
为解决这类问题，需要用其他方法对句子进行表示，LSTM是常用的一种方式，本文简单使用单层LSTM对句子重新表示，并通过若干全连接层对句子相似度进行衡量。</li>
</ul>
<h5 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h5><p> 训练和测试数据包括两个待比较句子以及其相似度（0-1）：<br><img src="http://upload-images.jianshu.io/upload_images/2159902-9c9f4574a6731b0a.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="train_pair"><br> 测试数据格式相似。</p>
<h5 id="语料编码"><a href="#语料编码" class="headerlink" title="语料编码"></a>语料编码</h5><p> 自然语言无法直接作为神经网络输入，需进行编码该部分包括以下步骤：</p>
<ul>
<li>读人训练和测试数据，分词，并给每个词编号。</li>
<li>根据词编号，进一步生成每个句子的编号向量，句子采用固定长度，不足的位置补零。</li>
<li>保存词编号到文件，保存词向量矩阵方便预测使用。</li>
</ul>
<p>中文分词使用jieba分词工具，词的编号则使用Keras的Tokenizer：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">print(&quot;Fit tokenizer...&quot;)</span><br><span class="line">tokenizer = Tokenizer(num_words=MAX_NB_WORDS, lower=False)</span><br><span class="line">tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)</span><br><span class="line">if save:</span><br><span class="line">    print(&quot;Save tokenizer...&quot;)</span><br><span class="line">    if not os.path.exists(save_path):</span><br><span class="line">        os.makedirs(save_path)</span><br><span class="line">    cPickle.dump(tokenizer, open(os.path.join(save_path, tokenizer_name), &quot;wb&quot;))</span><br></pre></td></tr></table></figure>
<p>其中texts_1 、texts_2 、test_texts_1 、 test_texts_2的元素分别为训练数据和测试数据的分词后的列表，如：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">[&quot;我&quot;， &quot;是&quot;, &quot;谁&quot;]</span><br></pre></td></tr></table></figure></p>
<p> 经过上面的过程 tokenizer保存了语料中出现过的词的编号映射。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt; print tokenizer.word_index</span><br><span class="line">&#123;&quot;我&quot;： 2， &quot;是&quot;：1， &quot;谁&quot;：3&#125;</span><br></pre></td></tr></table></figure></p>
<p>利用tokenizer对语料中的句子进行编号<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; sequences_1 = tokenizer.texts_to_sequences(texts_1)</span><br><span class="line">&gt; print sequences_1</span><br><span class="line">[[2 1 3], ...]</span><br></pre></td></tr></table></figure></p>
<p>最终生成固定长度(假设为10)的句子编号列表<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt; data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)</span><br><span class="line">&gt; print data_1</span><br><span class="line">[[0 0 0 0 0 0 0 2 1 3], ...]</span><br></pre></td></tr></table></figure></p>
<p> data_1即可作为神经网络的输入。</p>
<h5 id="词向量映射"><a href="#词向量映射" class="headerlink" title="词向量映射"></a>词向量映射</h5><p>在对句子进行编码后，需要准备句子中词的词向量映射作为LSTM层的输入。这里使用预训练的词向量（<a href="http://someth.duapp.com/2017/07/05/Word2vector%E5%8F%A5%E5%AD%90%E7%9B%B8%E4%BC%BC%E5%BA%A6%E8%AE%A1%E7%AE%97/" target="_blank" rel="noopener">这里</a>）参数，生成词向量映射矩阵：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">word2vec = Word2Vec.load(EMBEDDING_FILE)</span><br><span class="line">embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))</span><br><span class="line">for word, i in word_index.items():</span><br><span class="line">    if word in word2vec.wv.vocab:</span><br><span class="line">        embedding_matrix[i] = word2vec.wv.word_vec(word)</span><br><span class="line">np.save(embedding_matrix_path, embedding_matrix)</span><br></pre></td></tr></table></figure></p>
<h5 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h5><p> 该神经网络采用简单的单层LSTM+全连接层对数据进行训练，网络结构图：<br>  <img src="http://upload-images.jianshu.io/upload_images/2159902-7301eefac2d9094b.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="lstm"><br> 网络由Keras实现：<br><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_model</span><span class="params">()</span>:</span></span><br><span class="line">    embedding_layer = Embedding(nb_words,</span><br><span class="line">                                EMBEDDING_DIM,</span><br><span class="line">                                weights=[embedding_matrix],</span><br><span class="line">                                input_length=MAX_SEQUENCE_LENGTH,</span><br><span class="line">                                trainable=<span class="keyword">False</span>)</span><br><span class="line">    lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)</span><br><span class="line"></span><br><span class="line">    sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=<span class="string">'int32'</span>)</span><br><span class="line">    embedded_sequences_1 = embedding_layer(sequence_1_input)</span><br><span class="line">    y1 = lstm_layer(embedded_sequences_1)</span><br><span class="line"></span><br><span class="line">    sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype=<span class="string">'int32'</span>)</span><br><span class="line">    embedded_sequences_2 = embedding_layer(sequence_2_input)</span><br><span class="line">    y2 = lstm_layer(embedded_sequences_2)</span><br><span class="line"></span><br><span class="line">    merged = concatenate([y1, y2])</span><br><span class="line">    merged = Dropout(rate_drop_dense)(merged)</span><br><span class="line">    merged = BatchNormalization()(merged)</span><br><span class="line"></span><br><span class="line">    merged = Dense(num_dense, activation=act)(merged)</span><br><span class="line">    merged = Dropout(rate_drop_dense)(merged)</span><br><span class="line">    merged = BatchNormalization()(merged)</span><br><span class="line">    preds = Dense(<span class="number">1</span>, activation=<span class="string">'sigmoid'</span>)(merged)</span><br><span class="line"></span><br><span class="line">    model = Model(inputs=[sequence_1_input, sequence_2_input], \</span><br><span class="line">                  outputs=preds)</span><br><span class="line">    model.compile(loss=<span class="string">'binary_crossentropy'</span>,</span><br><span class="line">                  optimizer=<span class="string">'nadam'</span>,</span><br><span class="line">                  metrics=[<span class="string">'acc'</span>])</span><br><span class="line">    model.summary()</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure></p>
<p>该部分首先定义embedding_layer作为输入层和LSTM层的映射层，将输入的句子编码映射为词向量列表作为LSTM层的输入。两个LSTM的输出拼接后作为全连接层的输入，经过Dropout和BatchNormalization正则化，最终输出结果进行训练。</p>
<h5 id="训练与预测"><a href="#训练与预测" class="headerlink" title="训练与预测"></a>训练与预测</h5><p> 训练采用nAdam以及EarlyStopping，保存训练过程中验证集上效果最好的参数。最终对测试集进行预测。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">model = get_model()</span><br><span class="line">early_stopping = EarlyStopping(monitor=&apos;val_loss&apos;, patience=3)</span><br><span class="line">bst_model_path = STAMP + &apos;.h5&apos;</span><br><span class="line">model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)</span><br><span class="line"></span><br><span class="line">hist = model.fit([data_1, data_2], labels, \</span><br><span class="line">                 validation_data=([val_1, val_2], labels), \</span><br><span class="line">                 epochs=100, batch_size=10, shuffle=True, callbacks=[early_stopping, model_checkpoint])</span><br><span class="line">predicts = model.predict([data_1, data_2], batch_size=10, verbose=1)</span><br><span class="line">for i in range(len(test_ids)):</span><br><span class="line">    print &quot;t1: %s, t2: %s, score: %s&quot; % (test_1[i], test_2[i], predicts[i])</span><br></pre></td></tr></table></figure></p>
<h5 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h5><p> 该网络在Kaggle Quora数据集val验证可达到80%左右的准确率，应用于中文，由于数据集有限，产生了较大的过拟合。此外在Tokenizer.fit_on_texts应用于中文时，不支持Unicode编码，可以对其源码方法进行重写，加入Ascii字符和Unicode的转换。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&apos;&apos;&apos;</span><br><span class="line">this part is solve keras.preprocessing.text can not process unicode</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">def text_to_word_sequence(text,</span><br><span class="line">                          filters=&apos;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~\t\n&apos;,</span><br><span class="line">                          lower=True, split=&quot; &quot;):</span><br><span class="line">    if lower: text = text.lower()</span><br><span class="line">    if type(text) == unicode:</span><br><span class="line">        translate_table = &#123;ord(c): ord(t) for c, t in zip(filters, split * len(filters))&#125;</span><br><span class="line">    else:</span><br><span class="line">        translate_table = keras.maketrans(filters, split * len(filters))</span><br><span class="line">    text = text.translate(translate_table)</span><br><span class="line">    seq = text.split(split)</span><br><span class="line">    return [i for i in seq if i]</span><br><span class="line">    </span><br><span class="line">keras.preprocessing.text.text_to_word_sequence = text_to_word_sequence</span><br></pre></td></tr></table></figure></p>
<h5 id="项目源码https-github-com-zqhZY-semanaly"><a href="#项目源码https-github-com-zqhZY-semanaly" class="headerlink" title="项目源码https://github.com/zqhZY/semanaly/"></a>项目源码<a href="https://github.com/zqhZY/semanaly/" target="_blank" rel="noopener">https://github.com/zqhZY/semanaly/</a></h5><p>更多关注公众号：<br><img src="https://upload-images.jianshu.io/upload_images/2159902-e28220521b7ef499.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="wechat"></p>

        </div>
      </article>
    </div>
    <!-- Comments -->
    <div class="container">
      
<section id="comment">
  <!-- <h1 class="title">留言</h1> -->

  
</section>


    </div>
    <!-- Pre or Next -->
    <div class="nav-links">
      
        <div class="nav-previous">
          <a href="/2017/06/20/deep-learning参数初始化/" rel="prev"><span class="meta-arraw meta-arraw-left"></span> 上一页</a>
        </div>
      
      
        <div class="nav-next">
          <a href="/2017/05/20/Word2vector句子相似度计算/" rel="prev">下一页 <span class="meta-arraw meta-arraw-right"></span></a>
        </div>
      
    </div>

  </div>
</div>


  <!-- Footer -->
  <!-- Footer-widgets -->
<div class="footer-widgets">
  <div class="row inside-wrapper">
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">关于本站</h1>
        <div class="custom-widget-content">
          
          <p align="left">分享日常学习，传播核心价值观，传播正能量。</p>
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">与我联系</h1>
        <div class="widget-text">
          
            
              <a href="https://github.com/zqhZY" class="icon icon-github" target="_blank">github</a>
            
              <a href="zqingheng@163.com" class="icon icon-mail" target="_blank">mail</a>
            
          
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">站内搜索</h1>
        <div class="widget-text">
          <form onSubmit="return appDaily.submitSearch('')">
            <p>
              <input type="text" placeholder="search..." id="homeSearchInput">
            </p>
            <!-- <input type="submit" value="GO"> -->
          </form>
        </div>
      </aside>
    </div>
  </div>
</div>
<!-- Footer -->
<footer class="site-info">
  <p>
    <span>A Note &copy; 2018</span>
    
      <span class="split">|</span>
      <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> with Theme <a href="https://github.com/GallenHu/hexo-theme-Daily" target="_blank">Daily</a></span>
    
  </p>
</footer>


  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>





</body>

</html>
