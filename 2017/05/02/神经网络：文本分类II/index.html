<!DOCTYPE html>
<html lang="zh-CN">

<!-- Head tag -->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">

  <!--Description-->
  

  <!--Author-->
  
  <meta name="author" content="Zico">
  

  <!--Open Graph Title-->
  
      <meta property="og:title" content="神经网络：文本分类II"/>
  
  <!--Open Graph Description-->
  
  <!--Open Graph Site Name-->
  <meta property="og:site_name" content="A Note"/>
  <!--Type page-->
  
      <meta property="og:type" content="article" />
  
  <!--Page Cover-->
  

  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <!-- Title -->
  
  <title>神经网络：文本分类II - A Note</title>


  <link rel="shortcut icon" href="https://hexo.io/icon/favicon-96x96.png">

  <!-- Custom CSS/Sass -->
  <link rel="stylesheet" href="/css/style.css">

  <!----------------------------
  https://github.com/GallenHu/hexo-theme-Daily

 _____            _   _
|  __ \          (_) | |
| |  | |   __ _   _  | |  _   _
| |  | |  / _` | | | | | | | | |
| |__| | | (_| | | | | | | |_| |
|_____/   \__,_| |_| |_|  \__, |
                          __/ |
                         |___/

    --------------------------->

</head>


<body>

  <!-- Nav -->
  <header class="site-header">
  <div class="header-inside">
    <div class="logo">
      <a href="/" rel="home">
        
        <img src="https://hexo.io/logo.svg" alt="A Note" height="60">
        
      </a>
    </div>
    <!-- Navigation -->
    <nav class="navbar">
      <!-- Collect the nav links, forms, and other content for toggling -->
      <div class="collapse">
        <ul class="navbar-nav">
          
          
            <li>
              <a href="/.">
                
                  Home
                
              </a>
            </li>
          
            <li>
              <a href="/archives">
                
                  Archive
                
              </a>
            </li>
          
        </ul>
      </div>
      <!-- /.navbar-collapse -->
    </nav>
    <div class="button-wrap">
      <button class="menu-toggle">Primary Menu</button>
    </div>
  </div>
</header>


  <!-- Main Content -->
  <div class="content-area">
  <div class="post">
    <!-- Post Content -->
    <div class="container">
      <article>
        <!-- Title date & tags -->
        <div class="post-header">
          <h1 class="entry-title">
            神经网络：文本分类II
            
          </h1>
          <p class="posted-on">
          2017-05-02
          </p>
          <div class="tags-links">
            
              
                <a href="/tags/DNN/" rel="tag">
                  DNN
                </a>
              
                <a href="/tags/NLP/" rel="tag">
                  NLP
                </a>
              
            
          </div>
        </div>
        <!-- Post Main Content -->
        <div class="entry-content has_line_number">
          <p><img src="http://upload-images.jianshu.io/upload_images/2159902-233fa181ffaa44fc.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="tensorflow"></p>
<p> 本篇文章主要记录对之前用神经网络做文本识别的初步优化，进一步将准确率由原来的65%提高到80%，这里优化的几个方面包括：<br><a id="more"></a></p>
<ul>
<li>随机打乱训练数据</li>
<li>增加隐层，和验证集</li>
<li>正则化</li>
<li>对原数据进行PCA预处理</li>
<li>调节训练参数（迭代次数，batch大小等）</li>
</ul>
<h5 id="随机化训练数据"><a href="#随机化训练数据" class="headerlink" title="随机化训练数据"></a>随机化训练数据</h5><p> 观察训练数据集，发现训练集是按类别存储，读进内存后在仍然是按类别顺序存放。这样顺序取一部分作为验证集，很大程度上会减少一个类别的训练样本数，对该类别的预测准确率会有所下降。所以首先考虑打乱训练数据。<br> 在已经向量化的训练数据的基础上打乱数据，首先合并data和label，打乱后再将数据和标签分离为trian.txt和train_label.txt。这里可以直接使用shell命令：</p>
<p>1、将labels加到trian.txt的第一列<br><figure class="highlight plain"><figcaption><span>-d" " train_labels.txt train.txt > train_to_shuf.txt```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">2、随机打乱文件行</span><br><span class="line"></span><br><span class="line">```shuf train_to_shuf.txt -o train.txt</span><br></pre></td></tr></table></figure></p>
<p>3、 提取打乱后文件的第一列，保存到train_labels.txt<br><figure class="highlight plain"><figcaption><span>train.txt | awk '&#123;print $1&#125;' > train_labels.txt ```</span></figcaption><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">4、删除第一列label.</span><br><span class="line">```awk &apos;&#123;$1=&quot;&quot;;print $0&#125;&apos;  train.txt</span><br></pre></td></tr></table></figure></p>
<p>这样再次以相同方式训练，准确率由65%上升到75% 。</p>
<h5 id="改变网络结构，增加隐层"><a href="#改变网络结构，增加隐层" class="headerlink" title="改变网络结构，增加隐层"></a>改变网络结构，增加隐层</h5><p> 之前的网络直接对输入数据做softmax回归，这里考虑增加隐层，数量并加入验证集观察准确率的变化情况。这里加入一个隐层，隐层节点数为500，激励函数使用Relu。替换原来的网络结构，准确率进一步上升。</p>
<h5 id="正则化，改善过拟合"><a href="#正则化，改善过拟合" class="headerlink" title="正则化，改善过拟合"></a>正则化，改善过拟合</h5><p> 观察模型对训练集的拟合程度到90%+，而通过上步对训练数据的准确率为76%，一定程度上出现了过拟合的现象，这里在原有cost function中上加入正则项，希望减轻过拟合的现象。这里使用L2正则。连同上步部分的代码如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python</span><br><span class="line">#-*-coding:utf-8-*-</span><br><span class="line"></span><br><span class="line">LAYER_NODE1 = 500 # layer1 node num</span><br><span class="line">INPUT_NODE = 5000</span><br><span class="line">OUTPUT_NODE = 10</span><br><span class="line">REG_RATE = 0.01</span><br><span class="line"></span><br><span class="line">import tensorflow as tf</span><br><span class="line">from datasets import datasets</span><br><span class="line"></span><br><span class="line">def interface(inputs, w1, b1, w2,b2):</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">		compute forword progration result</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	lay1 = tf.nn.relu(tf.matmul(inputs, w1) + b1)</span><br><span class="line">	return tf.nn.softmax(tf.matmul(lay1, w2) + b2) # need softmax??</span><br><span class="line"></span><br><span class="line">data_sets = datasets()</span><br><span class="line">data_sets.read_train_data(&quot;.&quot;, True)</span><br><span class="line"></span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line">x = tf.placeholder(tf.float32, [None, INPUT_NODE], name=&quot;x-input&quot;)</span><br><span class="line">y_ = tf.placeholder(tf.float32, [None, OUTPUT_NODE], name=&quot;y-input&quot;)</span><br><span class="line"></span><br><span class="line">w1 = tf.Variable(tf.truncated_normal([INPUT_NODE, LAYER_NODE1], stddev=0.1))</span><br><span class="line">b1 = tf.Variable(tf.constant(0.0, shape=[LAYER_NODE1]))</span><br><span class="line"></span><br><span class="line">w2 = tf.Variable(tf.truncated_normal([LAYER_NODE1, OUTPUT_NODE], stddev=0.1))</span><br><span class="line">b2 = tf.Variable(tf.constant(0.0, shape=[OUTPUT_NODE]))</span><br><span class="line"></span><br><span class="line">y = interface(x, w1, b1, w2, b2)</span><br><span class="line"></span><br><span class="line">cross_entropy = -tf.reduce_sum(y_ * tf.log(y + 1e-10))</span><br><span class="line">regularizer = tf.contrib.layers.l2_regularizer(REG_RATE)</span><br><span class="line">regularization = regularizer(w1) + regularizer(w2)</span><br><span class="line">loss = cross_entropy + regularization</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_step = tf.train.GradientDescentOptimizer(0.01).minimize(loss)</span><br><span class="line"></span><br><span class="line">#training</span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line">saver = tf.train.Saver()</span><br><span class="line"></span><br><span class="line">cv_feed = &#123;x: data_sets.cv.text, y_: data_sets.cv.label&#125;</span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))</span><br><span class="line">acc = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line">for i in range(5000):</span><br><span class="line">	if i % 200 == 0:</span><br><span class="line">		cv_acc = sess.run(acc, feed_dict=cv_feed)</span><br><span class="line">		print &quot;train steps: %d, cv accuracy is %g &quot; % (i, cv_acc)</span><br><span class="line">	batch_xs, batch_ys = data_sets.train.next_batch(100)</span><br><span class="line">	train_step.run(&#123;x: batch_xs, y_: batch_ys&#125;)</span><br><span class="line"></span><br><span class="line">path = saver.save(sess, &quot;./model4/model.md&quot;)</span><br></pre></td></tr></table></figure></p>
<h5 id="PCA处理"><a href="#PCA处理" class="headerlink" title="PCA处理"></a>PCA处理</h5><p> 一方面对文本向量集是严重稀疏的矩阵，而且维度较大，一方面影响训练速度，一方面消耗内存。这里考虑对数据进行PCA处理。该部分希望保存99%的差异率，得到相应的k，即对应的维度。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python</span><br><span class="line">#-*-coding:utf-8-*-</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">	PCA for datasets</span><br><span class="line">	</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">import os</span><br><span class="line">import sys</span><br><span class="line">import commands</span><br><span class="line">import numpy</span><br><span class="line">from contextlib import nested</span><br><span class="line">from datasets import datasets</span><br><span class="line"></span><br><span class="line">ORIGIN_DIM = 5000</span><br><span class="line"></span><br><span class="line">def pca(origin_mat):</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">		gen matrix using pca</span><br><span class="line">		row of origin_mat is one sample of dataset</span><br><span class="line">		col of origin_mat is one feature</span><br><span class="line">		return matrix  U, s and  V </span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	# mean,normaliza1on</span><br><span class="line">	avg = numpy.mean(origin_mat, axis=0)</span><br><span class="line">	# covariance matrix</span><br><span class="line">	cov = numpy.cov(origin_mat-avg,rowvar=0)</span><br><span class="line">	#Singular Value Decomposition</span><br><span class="line">	U, s, V = numpy.linalg.svd(cov, full_matrices=True)</span><br><span class="line"></span><br><span class="line">	k = 1;</span><br><span class="line">	sigma_s = numpy.sum(s)</span><br><span class="line">	# chose smallest k for 99% of variance retained </span><br><span class="line">	for k in range(1, ORIGIN_DIM+1):</span><br><span class="line">		variance = numpy.sum(s[0:k]) / sigma_s</span><br><span class="line">		print &quot;k = %d, variance is %f&quot; % (k, variance)</span><br><span class="line">		if variance &gt;= 0.99:</span><br><span class="line">			break</span><br><span class="line"></span><br><span class="line">	if k == ORIGIN_DIM:</span><br><span class="line">		print &quot;some thing unexpected , k is same as ORIGIN_DIM&quot;</span><br><span class="line">		exit(1)</span><br><span class="line"></span><br><span class="line">	return U[:, 0:k], k</span><br><span class="line"></span><br><span class="line">if __name__ == &apos;__main__&apos;:</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">		main, read train.txt, and do pca</span><br><span class="line">		save file to train_pca.txt</span><br><span class="line">	&quot;&quot;&quot;</span><br><span class="line">	data_sets = datasets()</span><br><span class="line">	train_text, _ = data_sets.read_from_disk(&quot;.&quot;, &quot;train&quot;, one_hot=False)</span><br><span class="line"></span><br><span class="line">	U, k = pca(train_text)</span><br><span class="line">	print &quot;U shpae: &quot;, U.shape</span><br><span class="line">	print &quot;k is : &quot;, k</span><br><span class="line"></span><br><span class="line">	text_pca = numpy.dot(train_text, U)</span><br><span class="line">	text_num = text_pca.shape[0]</span><br><span class="line">	print &quot;text_num in pca is &quot;, text_num</span><br><span class="line"></span><br><span class="line">	with open(&quot;./train_pca.txt&quot;, &quot;a+&quot;) as f:</span><br><span class="line">		for i in range(0, text_num):</span><br><span class="line">			f.write(&quot; &quot;.join(map(str, text_pca[i,:])) + &quot;\n&quot;)</span><br></pre></td></tr></table></figure></p>
<p>最终得到k=2583。该部分准确率有所提高但影响不大。</p>
<h5 id="调整网络参数"><a href="#调整网络参数" class="headerlink" title="调整网络参数"></a>调整网络参数</h5><p> 该部分主要根据严重集和测试集的表现不断调整网路参数，包括学习率、网路层数、每层节点个数、正则损失、迭代次数、batch大小等。最终得到80%的准确率。</p>
<h4 id="小结"><a href="#小结" class="headerlink" title="小结"></a>小结</h4><p>对神经网路进行初步优化，由原来的65%的准确率提高到80%，主要的提高在于训练数据的随机化，以及网络结构的调整。为提升训练速度，同时减少内存消耗，对数据进行了降维操作。</p>
<p>之后对代码的结构进行了整理，这里没有提及，该部分代码包括<a href="https://github.com/zqhZY/textclasser/blob/master/src/nn_interface.py" target="_blank" rel="noopener">nn_interface.py</a>和<a href="https://github.com/zqhZY/textclasser/blob/master/src/nn_train.py" target="_blank" rel="noopener">nn_train.py</a> 分别实现对网络结构的定义以及训练流程的管理。</p>
<p>后面会结合tensorflow的使用技巧对训练进行进一步优化。</p>
<p>更多关注公众号：<br><img src="https://upload-images.jianshu.io/upload_images/2159902-e28220521b7ef499.jpg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="wechat"></p>

        </div>
      </article>
    </div>
    <!-- Comments -->
    <div class="container">
      
<section id="comment">
  <!-- <h1 class="title">留言</h1> -->

  
</section>


    </div>
    <!-- Pre or Next -->
    <div class="nav-links">
      
        <div class="nav-previous">
          <a href="/2017/05/20/Word2vector句子相似度计算/" rel="prev"><span class="meta-arraw meta-arraw-left"></span> 上一页</a>
        </div>
      
      
        <div class="nav-next">
          <a href="/2017/05/01/神经网络：文本分类/" rel="prev">下一页 <span class="meta-arraw meta-arraw-right"></span></a>
        </div>
      
    </div>

  </div>
</div>


  <!-- Footer -->
  <!-- Footer-widgets -->
<div class="footer-widgets">
  <div class="row inside-wrapper">
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">关于本站</h1>
        <div class="custom-widget-content">
          
          <p align="left">分享日常学习，传播核心价值观，传播正能量。</p>
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">与我联系</h1>
        <div class="widget-text">
          
            
              <a href="https://github.com/zqhZY" class="icon icon-github" target="_blank">github</a>
            
              <a href="zqingheng@163.com" class="icon icon-mail" target="_blank">mail</a>
            
          
        </div>
      </aside>
    </div>
    <div class="col-1-3">
      <aside>
        <h1 class="widget-title">站内搜索</h1>
        <div class="widget-text">
          <form onSubmit="return appDaily.submitSearch('')">
            <p>
              <input type="text" placeholder="search..." id="homeSearchInput">
            </p>
            <!-- <input type="submit" value="GO"> -->
          </form>
        </div>
      </aside>
    </div>
  </div>
</div>
<!-- Footer -->
<footer class="site-info">
  <p>
    <span>A Note &copy; 2018</span>
    
      <span class="split">|</span>
      <span>Powered by <a href="https://hexo.io/" target="_blank">Hexo</a> with Theme <a href="https://github.com/GallenHu/hexo-theme-Daily" target="_blank">Daily</a></span>
    
  </p>
</footer>


  <!-- After footer scripts -->
  <!-- scripts -->
<script src="/js/app.js"></script>





</body>

</html>
